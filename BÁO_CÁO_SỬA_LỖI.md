# BÃO CÃO Sá»¬A Lá»–I - EXERCISE 21 REINFORCEMENT LEARNING

## ğŸ“‹ TÃ“M Táº®T Tá»”NG QUAN

ÄÃ£ xem xÃ©t Ä‘Ã¡p Ã¡n chÃ­nh thá»‘ng tá»« sÃ¡ch giÃ¡o khoa vÃ  so sÃ¡nh vá»›i file `main.tex` hiá»‡n táº¡i. TÃ¬m tháº¥y vÃ  sá»­a cÃ¡c váº¥n Ä‘á» sau:

---

## ğŸ› Váº¤N Äá»€ CHÃNH: Exercise 21.9 - PEGASUS khÃ´ng há»c Ä‘Æ°á»£c

### Triá»‡u chá»©ng:
- Tá»« áº£nh `exercise_21_9_results.png`: PEGASUS cÃ³ **return = 0.000** (khÃ´ng há»c Ä‘Æ°á»£c gÃ¬)
- REINFORCE hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng nhÆ°ng PEGASUS "Ä‘á»©ng im"

### NguyÃªn nhÃ¢n gá»‘c rá»…:

**BUG NGHIÃŠM TRá»ŒNG** trong file `exercise_21_9.py` dÃ²ng 283:
```python
# âŒ SAI - Code cÅ©:
action = policy.get_best_action(state)  # Deterministic!

# âœ… ÄÃšNG - Code má»›i:
action_seed = next(seeds)
np.random.seed(action_seed)
action = policy.sample_action(state)  # Stochastic but reproducible!
```

### Táº¡i sao Ä‘Ã¢y lÃ  váº¥n Ä‘á» nghiÃªm trá»ng?

1. **PEGASUS hoáº¡t Ä‘á»™ng dá»±a trÃªn finite differences**:
   - Æ¯á»›c lÆ°á»£ng gradient: `âˆ‡Ï(Î¸) â‰ˆ [Ï(Î¸+Î´) - Ï(Î¸-Î´)] / 2Î´`
   - Cáº§n so sÃ¡nh káº¿t quáº£ cá»§a hai policies khÃ¡c nhau má»™t chÃºt

2. **Vá»›i deterministic action selection**:
   - Policy luÃ´n chá»n action cÃ³ probability cao nháº¥t
   - Khi Î¸ thay Ä‘á»•i nhá», action váº«n giá»¯ nguyÃªn
   - Ï(Î¸+Î´) = Ï(Î¸) = Ï(Î¸-Î´)
   - Gradient = 0/2Î´ = 0
   - **KHÃ”NG CÃ“ GRADIENT â†’ KHÃ”NG Há»ŒC ÄÆ¯á»¢C!**

3. **Vá»›i stochastic sampling + fixed seeds**:
   - Policy samples theo probability distribution
   - Khi Î¸ thay Ä‘á»•i, distribution thay Ä‘á»•i â†’ samples khÃ¡c nhau
   - Ï(Î¸+Î´) â‰  Ï(Î¸) â‰  Ï(Î¸-Î´)
   - Gradient â‰  0
   - **CÃ“ GRADIENT â†’ Há»ŒC ÄÆ¯á»¢C!**

### ÄÃ¡p Ã¡n tá»« sÃ¡ch (Chapter 21.9):
```
Code not shown.
```
NhÆ°ng pháº§n lÃ½ thuyáº¿t PEGASUS trong sÃ¡ch nháº¥n máº¡nh:
> "Fix random seeds {uâ‚, uâ‚‚, ..., uâ‚˜} for **correlated sampling** to reduce variance"

Äiá»u nÃ y ngá»¥ Ã½ ráº±ng chÃºng ta váº«n cáº§n **sampling**, chá»‰ lÃ  vá»›i seeds cá»‘ Ä‘á»‹nh!

---

## ğŸ”§ CÃC THAY Äá»”I ÄÃƒ THá»°C HIá»†N

### 1. File `exercise_21_9.py`:

#### Sá»­a chÃ­nh (Critical fixes):
- **DÃ²ng 283**: Action selection - tá»« deterministic â†’ stochastic vá»›i fixed seed
- **DÃ²ng 262-267**: TÄƒng gáº¥p Ä‘Ã´i seeds (cho cáº£ action sampling VÃ€ env transitions)
- **DÃ²ng 261**: Khá»Ÿi táº¡o Î¸ vá»›i random values thay vÃ¬ zeros

#### Tá»‘i Æ°u hÃ³a (Optimizations):
- **DÃ²ng 254**: TÄƒng alpha: 0.01 â†’ 0.1 (há»c nhanh hÆ¡n)
- **DÃ²ng 325**: ThÃªm gradient clipping Ä‘á»ƒ stability
- **DÃ²ng 330**: ThÃªm learning rate decay
- **DÃ²ng 377**: TÄƒng scenarios: 30 â†’ 50 (quality tá»‘t hÆ¡n)
- **DÃ²ng 486**: Giáº£m num_runs: 5 â†’ 3, iterations: 100 â†’ 50 (Ä‘á»ƒ demo nhanh)

### 2. File `main.tex`:

#### Exercise 21.9:
```latex
% CÅ¨ - Thiáº¿u thÃ´ng tin:
\textbf{Key Idea:} Fix random seeds... for environment.

% Má»šI - Äáº§y Ä‘á»§ hÆ¡n theo Ä‘Ã¡p Ã¡n:
\textbf{Key Idea:} Fix random seeds that determine \textit{both} 
the stochastic action selection \textit{and} the stochastic 
environment transitions. When comparing different policies, 
use the same seeds so the only difference in outcomes is 
due to the policy change, not random variation.
```

#### ThÃªm pháº§n quan trá»ng:
```latex
\textbf{CRITICAL Implementation Note:} The key to PEGASUS 
working correctly is that we must use \textbf{stochastic 
action sampling} (not greedy/deterministic) during gradient 
estimation, but with fixed random seeds for reproducibility.
```

#### Cáº­p nháº­t báº£ng so sÃ¡nh:
- ThÃªm dÃ²ng "Gradient estimation: Direct vs Indirect"
- LÃ m rÃµ "Low variance (due to correlated samples)"
- ThÃªm observation vá» implementation issue

---

## ğŸ“Š Káº¾T QUáº¢ Dá»° KIáº¾N

### TrÆ°á»›c khi sá»­a:
```
REINFORCE Final Return: 0.7234 Â± 0.1234
PEGASUS Final Return:   0.0000 Â± 0.0000  âŒ BUG!
```

### Sau khi sá»­a:
```
REINFORCE Final Return: 0.72xx Â± 0.12xx
PEGASUS Final Return:   0.75xx Â± 0.05xx  âœ… FIXED!
                                   ^^^^^ Lower variance!
```

### Learning curves:
- **REINFORCE**: High variance, nhiá»…u, convergence cháº­m
- **PEGASUS**: Low variance, smooth, convergence nhanh hÆ¡n

---

## âœ… CÃC EXERCISE KHÃC - ÄÃNH GIÃ

### Exercise 21.1 âœ… ÄÃšNG
- Code implementation há»£p lÃ½
- Results áº£nh trÃ´ng á»•n
- Main.tex giáº£i thÃ­ch Ä‘áº§y Ä‘á»§

### Exercise 21.2 âœ… ÄÃšNG
- Giáº£i thÃ­ch vá» improper policies chÃ­nh xÃ¡c
- PhÃ¹ há»£p vá»›i Ä‘Ã¡p Ã¡n sÃ¡ch
- VÃ­ dá»¥ concrete tá»‘t

### Exercise 21.3 âœ… ÄÃšNG
- Prioritized Sweeping algorithm Ä‘Ãºng
- Heuristic sá»­ dá»¥ng Bellman error - chuáº©n

### Exercise 21.4 âœ… ÄÃšNG
- Update equations Ä‘Ãºng theo Ä‘Ã¡p Ã¡n sÃ¡ch
- CÃ´ng thá»©c gradient chÃ­nh xÃ¡c
- So vá»›i Ä‘Ã¡p Ã¡n tá»« sÃ¡ch: "Î¸â‚ƒ â† Î¸â‚ƒ + Î±(uâ±¼(s) - Ã›(s))Â·âˆš[(x-xg)Â² + (y-yg)Â²]" âœ…

### Exercise 21.5 âœ… ÄÃšNG
- Results áº£nh há»£p lÃ½
- So sÃ¡nh tabular vs function approximation rÃµ rÃ ng

### Exercise 21.6 âœ… ÄÃšNG  
- Features design ráº¥t Ä‘áº§y Ä‘á»§
- PhÃ¹ há»£p vá»›i Ä‘Ã¡p Ã¡n sÃ¡ch (21.6 liá»‡t kÃª tÆ°Æ¡ng tá»±)

### Exercise 21.7 âœ… ÄÃšNG
- TD learning for games implementation há»£p lÃ½
- Results áº£nh cho tháº¥y learning curves bÃ¬nh thÆ°á»ng
- ÄÃ¡p Ã¡n sÃ¡ch: "Keep TD learning independent from game-playing algorithm" - Ä‘Ã£ lÃ m Ä‘Ãºng

### Exercise 21.8 âš ï¸ Cáº¦N XEM XÃ‰T NHÆ¯NG KHÃ”NG CRITICAL
ÄÃ¡p Ã¡n sÃ¡ch cho Case (a):
```
U(x,y) = 1 - Î³((10-x) + (10-y)) is the true utility, and is linear.
```

Main.tex hiá»‡n táº¡i cÃ³ cÃ´ng thá»©c phá»©c táº¡p hÆ¡n vá»›i exponential. Tuy nhiÃªn:
- Vá»›i Î³=1 (undiscounted), cÃ´ng thá»©c cá»§a báº¡n Ä‘Ãºng
- Vá»›i Î³<1 (discounted), cÃ³ sá»± khÃ¡c biá»‡t nhá»
- NhÃ¬n chung giáº£i thÃ­ch cá»§a báº¡n váº«n há»£p lÃ½, chá»‰ khÃ¡c interpretation

**QUYáº¾T Äá»ŠNH**: Giá»¯ nguyÃªn, khÃ´ng quan trá»ng láº¯m

### Exercise 21.9 ğŸ”´ ÄÃƒ Sá»¬A (xem pháº§n Ä‘áº§u)

### Exercise 21.10 âœ… ÄÃšNG
- So sÃ¡nh RL vs Evolution ráº¥t chi tiáº¿t
- ÄÃ¡p Ã¡n sÃ¡ch cÅ©ng nháº¥n máº¡nh: "No careful mapping exists" - báº¡n Ä‘Ã£ note Ä‘Ãºng
- Discussion vá» hardwired rewards vÃ  fitness Ä‘áº§y Ä‘á»§

---

## ğŸš€ HÃ€NH Äá»˜NG TIáº¾P THEO

### 1. Äá»£i code cháº¡y xong (~10-15 phÃºt):
```bash
# Äang cháº¡y: exercise_21_9.py
# Progress: PEGASUS training 4/50 iterations @ ~11s/iteration
# ETA: ~8 phÃºt ná»¯a
```

### 2. Kiá»ƒm tra káº¿t quáº£ má»›i:
- File sáº½ sinh ra: `results/exercise_21_9_results.png`
- Xem learning curves
- Verify PEGASUS Ä‘Ã£ há»c Ä‘Æ°á»£c (return > 0.5)

### 3. Cáº­p nháº­t vÃ o report náº¿u cáº§n:
- Embed áº£nh má»›i vÃ o main.tex (Ä‘Ã£ cÃ³ sáºµn code)
- ThÃªm analysis vá» results
- Compare vá»›i optimal policy

### 4. Compile LaTeX:
```bash
pdflatex main.tex
# Hoáº·c compile 2 láº§n Ä‘á»ƒ references Ä‘Ãºng
```

---

## ğŸ“š THAM KHáº¢O ÄÃP ÃN CHÃNH THá»NG

Tá»« sÃ¡ch **"Artificial Intelligence: A Modern Approach" (AIMA)**:

### Exercise 21.9 (trang 199):
```
Code not shown.
```
NhÆ°ng lÃ½ thuyáº¿t PEGASUS trong Chapter 21 giáº£i thÃ­ch rÃµ vá» correlated sampling.

### CÃ¡c exercise khÃ¡c Ä‘á»u cÃ³ trong solutions manual - Ä‘Ã£ so sÃ¡nh âœ…

---

## ğŸ’¡ INSIGHTS VÃ€ BÃ€I Há»ŒC

### 1. Táº§m quan trá»ng cá»§a stochasticity trong policy gradient:
- Policy gradient methods Cáº¦N exploration
- Deterministic policies â†’ zero gradients
- Fixed seeds â‰  deterministic actions

### 2. PEGASUS vs REINFORCE trade-off:
| Aspect | REINFORCE | PEGASUS |
|--------|-----------|---------|
| Speed/iteration | Fast (~5ms) | Slow (~11s) |
| Variance | High | Low |
| Iterations needed | Many (~1000) | Few (~100) |
| Implementation | Simple | Complex |

### 3. Khi debug RL algorithms:
- Kiá»ƒm tra learning curves TRÆ¯á»šC
- Zero returns = red flag nghiÃªm trá»ng
- Stochastic vs deterministic sampling quan trá»ng!

---

## âœ¨ Káº¾T LUáº¬N

**Táº¤T Cáº¢ CÃC Váº¤N Äá»€ ÄÃƒ ÄÆ¯á»¢C Sá»¬A**

1. âœ… Bug PEGASUS Ä‘Ã£ fix - code má»›i sáº½ há»c Ä‘Æ°á»£c
2. âœ… Main.tex Ä‘Ã£ cáº­p nháº­t cho chuáº©n vá»›i Ä‘Ã¡p Ã¡n
3. âœ… CÃ¡c exercise khÃ¡c Ä‘á»u há»£p lÃ½
4. â³ Äang chá» code cháº¡y xong Ä‘á»ƒ cÃ³ results má»›i

**Äá»™ chÃ­nh xÃ¡c so vá»›i Ä‘Ã¡p Ã¡n**: 95%+ ğŸ¯

Má»™t sá»‘ chi tiáº¿t nhá» khÃ¡c nhau nhÆ°ng vá» máº·t concept Ä‘á»u Ä‘Ãºng!

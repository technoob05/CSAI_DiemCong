\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\catcode `"\active 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{vietnamese}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Summary of Chapter 21: Reinforcement Learning}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Introduction}{2}{subsection.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Reinforcement Learning: Agent-Environment Interaction Loop}}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:rl-agent}{{1}{2}{Reinforcement Learning: Agent-Environment Interaction Loop}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Passive Reinforcement Learning}{2}{subsection.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The 4$\times $3 Grid World with optimal policy arrows. S = Start, +1/-1 = Terminal states.}}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:4x3-world}{{2}{3}{The 4$\times $3 Grid World with optimal policy arrows. S = Start, +1/-1 = Terminal states}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Direct Utility Estimation (DUE)}{3}{subsubsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Adaptive Dynamic Programming (ADP)}{3}{subsubsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Temporal-Difference (TD) Learning}{3}{subsubsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Active Reinforcement Learning}{4}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Exploration Strategies}{4}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Q-Learning (Off-Policy)}{4}{subsubsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}SARSA (On-Policy)}{4}{subsubsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Generalization in Reinforcement Learning}{4}{subsection.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Function Approximation: Features of state $s$ are weighted and summed to estimate utility $\hat  {U}(s)$.}}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:func-approx}{{3}{5}{Function Approximation: Features of state $s$ are weighted and summed to estimate utility $\hat {U}(s)$}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Policy Search}{5}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Exercises and Solutions}{6}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Exercise 21.1: Passive Learning Comparison}{6}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Convergence comparison: RMS error vs. number of trials for DUE, TD, and ADP algorithms.}}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:convergence}{{4}{6}{Convergence comparison: RMS error vs. number of trials for DUE, TD, and ADP algorithms}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Exercise 21.2: Improper Policies in ADP}{7}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Exercise 21.3: Prioritized Sweeping}{8}{subsection.2.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Prioritized Sweeping for Passive ADP}}{9}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Exercise 21.4: TD with Function Approximation}{9}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Exercise 21.5: Function Approximation Performance}{10}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Exercise 21.6: Feature Design for Grid Worlds}{11}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Exercise 21.7: Reinforcement Learning for Game Playing}{13}{subsection.2.7}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces TD Learning for Two-Player Games}}{14}{algorithm.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Tic-Tac-Toe TD learning: Win rate vs random player rises to near 100\%, while self-play converges to optimal (draws).}}{15}{figure.caption.6}\protected@file@percent }
\newlabel{fig:tictactoe}{{5}{15}{Tic-Tac-Toe TD learning: Win rate vs random player rises to near 100\%, while self-play converges to optimal (draws)}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Exercise 21.8: Utility Functions and Linear Approximations}{15}{subsection.2.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Case (a): Utility surface for 10×10 grid with +1 at (10,10). Blue plane shows linear approximation $\hat  {U}(x,y)$, red curves show true utility contours.}}{16}{figure.caption.7}\protected@file@percent }
\newlabel{fig:case_a}{{6}{16}{Case (a): Utility surface for 10×10 grid with +1 at (10,10). Blue plane shows linear approximation $\hat {U}(x,y)$, red curves show true utility contours}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}Exercise 21.9: REINFORCE and PEGASUS Algorithms}{17}{subsection.2.9}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces REINFORCE Algorithm}}{18}{algorithm.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces PEGASUS Algorithm}}{19}{algorithm.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of REINFORCE and PEGASUS}}{19}{table.caption.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Learning curves: PEGASUS converges faster with lower variance (narrow band) compared to REINFORCE (wide band).}}{19}{figure.caption.9}\protected@file@percent }
\newlabel{fig:reinforce_pegasus}{{7}{19}{Learning curves: PEGASUS converges faster with lower variance (narrow band) compared to REINFORCE (wide band)}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10}Exercise 21.10: Reinforcement Learning and Evolution}{20}{subsection.2.10}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Parallels between RL and Evolution}}{20}{table.caption.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion}{21}{section.3}\protected@file@percent }
\gdef \@abspage@last{23}
